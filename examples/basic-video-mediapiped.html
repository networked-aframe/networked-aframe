<html>
  <head>
    <meta charset="utf-8">
    <title>Video Streaming Example — Networked-Aframe</title>
    <meta name="description" content="Dev Example — Networked-Aframe">

    <script src="https://aframe.io/releases/1.2.0/aframe.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.3.0/socket.io.slim.js"></script>
    <script src="/easyrtc/easyrtc.js"></script>
    <script src="/dist/networked-aframe.js"></script>

    <script src="https://unpkg.com/aframe-randomizer-components@^3.0.1/dist/aframe-randomizer-components.min.js"></script>

    <script src="/js/spawn-in-circle.component.js"></script>
    <link rel="stylesheet" type="text/css" href="/css/style.css" />



    <!-- Selfie Segmentation of Mediapipe-->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@0.3/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils@0.6/control_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@0.3/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation@0.1/selfie_segmentation.js" crossorigin="anonymous"></script>


  </head>
  <body>

  <!-- A video canvas for checking MediaPipe segmentation -->
  <div class="videoUI">
    <span id="live_label">Selfie live Transmission</span>
    <div class="video-canvas-container">

      <canvas id="video_output_canvas" width="320px" height="240px" style="width:160px; height:120px">
      </canvas>
    </div>
    <!-- This control panel is needed by mediapipe to ignite. I hide it. -->
    <div class="control-panel" style="visibility: hidden"></div>
  </div>


    <a-scene networked-scene="
      room: basic-video-mediapiped;
      debug: true;
      adapter: easyrtc;
      audio: false;
      video: true;
      connectOnLoad: false;
    ">
      <a-assets>

        <img id="grid" src="https://img.gs/bbdkhfbzkk/stretch/https://i.imgur.com/25P1geh.png" crossorigin="anonymous">
        <img id="sky" src="https://i.imgur.com/WqlqEkq.jpg" crossorigin="anonymous" />

        <!-- Templates -->

        <!-- Avatar -->
        <template id="avatar-template">
          <a-entity class="avatar">
             <a-plane  color="#FFF" width="4" height="3" position="0 .6 0"  material="side: front" networked-video-source-mediapiped></a-plane>
          </a-entity>
        </template>

        <!-- /Templates -->
      </a-assets>

      <a-entity id="player"
        networked="template:#avatar-template;attachTemplateToLocal:false;"
        camera
        position="0 1.6 0"
        spawn-in-circle="radius:3"
        wasd-controls look-controls
        >
        <a-sphere class="head"
          visible="false"
          random-color
        ></a-sphere>
      </a-entity>

      <a-entity position="0 0 0"
        geometry="primitive: plane; width: 10000; height: 10000;" rotation="-90 0 0"
        material="src: #grid; repeat: 10000 10000; transparent: true; metalness:0.6; roughness: 0.4; sphericalEnvMap: #sky;"></a-entity>

      <a-entity light="color: #ccccff; intensity: 1; type: ambient;" visible=""></a-entity>
      <a-entity light="color: #ffaaff; intensity: 1.5" position="5 5 5"></a-entity>

      <a-sky src="#sky" rotation="0 -90 0"></a-sky>

    </a-scene>

    <!-- GitHub Corner. -->
    <a href="https://github.com/networked-aframe/networked-aframe" class="github-corner">
      <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#222; color:#fff; position: absolute; top: 0; border: 0; right: 0;">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
      </svg>
    </a>
    <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}
    </style>

    <div class="actions">
      <button id="camera-btn" type="button" class="button">Hide Camera</button>
    </div>

    <script>
      // Camera status
      let cameraEnabled = true;
      // Camera button element
      const cameraBtnEle = document.getElementById('camera-btn');

      // On mobile remove elements that are resource heavy
      const isMobile = AFRAME.utils.device.isMobile();



      // Define custom schema for syncing avatar color, set by random-color
      NAF.schemas.add({
        template: '#avatar-template',
        components: [
          'position',
          'rotation'
        ]
      });

      // Called by Networked-Aframe when connected to server
      function onConnect () {
        console.log("onConnect", new Date());

        // Handle camera button click (Off and On)
        cameraBtnEle.addEventListener('click', function() {
          NAF.connection.adapter.enableCamera(!cameraEnabled);
          cameraEnabled = !cameraEnabled;
          cameraBtnEle.textContent = cameraEnabled ? 'Hide Camera' : 'Show Camera';
        });
      }

      var scene = document.querySelector('a-scene');

      if (scene.hasLoaded) {
        onSceneLoad();
      } else {
        const passive = { passive : true };
        scene.addEventListener('loaded', onSceneLoad, passive);
      }

      function onSceneLoad() {

        var canvasElement = document.getElementById('video_output_canvas');

        var canvasCtx = canvasElement.getContext('2d');
        const w = canvasElement.width;
        const h = canvasElement.height;

        // Mask
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

        var canvasStream = canvasElement.captureStream(30);

        document.querySelector('a-scene').components['networked-scene'].data.capturedCanvasStreamSource = null; //canvasStream;

        setTimeout(function(){document.querySelector('a-scene').components['networked-scene'].connect();}, 5000);
      }



      <!-- Texture assign Selfie Segmentation -->
      AFRAME.registerComponent('networked-video-source-mediapiped', {
        schema: {},
        dependencies: ['material'],
        init: function () {


            this.videoTexture = null;
            this.video = null;
            this.stream = null;
            this._setMediaStream = this._setMediaStream.bind(this);

            NAF.utils.getNetworkedEntity(this.el).then((networkedEl) => {
              const ownerId = networkedEl.components.networked.data.owner;
              if (ownerId) {
                NAF.connection.adapter.getMediaStream(ownerId, "video")
                        .then(this._setMediaStream)
                        .catch((e) => NAF.log.error(`Error getting media stream for ${ownerId}`, e));
              } else {
                // Correctly configured local entity, perhaps do something here for enabling debug audio loopback
              }
            });

        },

        _setMediaStream(newStream) {
          if (!this.video) {
            this.setupVideo();
          }

          if (newStream != this.stream) {

            if (this.stream) {
              this._clearMediaStream();
            }

            if (newStream) {
              console.log("shown video track = >>> ", newStream.getVideoTracks());
              let mMs = new MediaStream();

              // ToDo: Fix this
              mMs.addTrack(newStream.getVideoTracks()[1]);


              this.video.srcObject = mMs;
              this.videoTexture = new THREE.VideoTexture(this.video);


              this.videoTexture.format = THREE.RGBAFormat;

              const mesh = this.el.getObject3D('mesh');

              let uniforms = {};
              uniforms.uMap = {type: 't', value: this.videoTexture}
              uniforms = THREE.UniformsUtils.merge([
                uniforms,
                THREE.UniformsLib['lights']
              ]);

              let materialIncoming = new THREE.ShaderMaterial({
                uniforms: uniforms
              })

              materialIncoming.vertexShader = `
                         varying vec2 vUv;

                        void main() {
                            vec4 worldPosition = modelViewMatrix * vec4( position, 1.0 );
                            vec3 vWorldPosition = worldPosition.xyz;
                            vUv = uv;
                            gl_Position = projectionMatrix * modelViewMatrix * vec4( position, 1.0 );
                        }
                      `;

              materialIncoming.fragmentShader = `
                           varying vec2 vUv;
                           uniform sampler2D uMap;

                           void main() {
                                vec2 uv = vUv;
                                vec4 tex1 = texture2D(uMap, uv * 1.0);
                                 if (tex1.g - tex1.r > 0.15 && tex1.g - tex1.r > 0.15)
                                    gl_FragColor = vec4(0,0,0,0);
                                 else
                                    gl_FragColor = vec4(tex1.r,tex1.g,tex1.b,1.0);
                            }
                      `;

              materialIncoming.transparent = true;
              materialIncoming.side = THREE.DoubleSide;
              mesh.material = materialIncoming;
              mesh.material.needsUpdate = true;

            }
            this.stream = newStream;
          }
        },

        _clearMediaStream() {

          this.stream = null;

          if (this.videoTexture) {

            if (this.videoTexture.image instanceof HTMLVideoElement) {
              // Note: this.videoTexture.image === this.video
              const video = this.videoTexture.image;
              video.pause();
              video.srcObject = null;
              video.load();
            }

            this.videoTexture.dispose();
            this.videoTexture = null;
          }
        },

        remove: function () {
          this._clearMediaStream();
        },

        setupVideo: function () {
          if (!this.video) {
            const video = document.createElement('video');
            video.setAttribute('autoplay', true);
            video.setAttribute('playsinline', true);
            video.setAttribute('muted', true);
            this.video = video;
          }
        }
      });





      // Mediapipe control panel
      const controls = window;

      //const mpSelfieSegmentation = window;
      const examples = {
        images: [],
        // {name: 'name', src: 'https://url.com'},
        videos: [],
      };
      const fpsControl = new controls.FPS();
      let activeEffect = 'background';
      const controlsElement = document.getElementsByClassName('control-panel')[0];

      var canvasElement = document.getElementById('video_output_canvas');
      var canvasCtx = canvasElement.getContext('2d');

      function drawResults(results) {
        //fpsControl.tick();
        const w = canvasElement.width;
        const h = canvasElement.height;

        // Mask
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        canvasCtx.drawImage(results.segmentationMask, 0, 0, w, h);

        // Image
        canvasCtx.globalCompositeOperation = 'source-in';
        canvasCtx.drawImage(results.image, 0, 0, w, h);

        // Background as green because webrtc can not pass through transparent pixels
        canvasCtx.fillStyle = "#00FF00FF";
        canvasCtx.globalCompositeOperation = 'xor';
        canvasCtx.fillRect(0, 0, w, h);

        // Image again in the back because it was lost in xor
        canvasCtx.globalCompositeOperation = 'destination-over';
        canvasCtx.drawImage(results.image, 0, 0, w, h);
      }

      //window.requestAnimationFrame(drawResults);
      const selfieSegmentation = new SelfieSegmentation({
        locateFile: (file) => {
          console.log(file);
          return `https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation@0.1/${file}`;
        }
      });

      selfieSegmentation.onResults(drawResults);

      // start capturing selfie into a stream
      var canvasStream = canvasElement.captureStream(30);

      new controls
              .ControlPanel(controlsElement, {
                selfieMode: true,
                modelSelection: 1,
                effect: 'background',
              })
              .add([
                new controls.StaticText({title: 'MediaPipe Selfie Segmentation'}),
                fpsControl,
                new controls.Toggle({title: 'Selfie Mode', field: 'selfieMode'}),
                new controls.SourcePicker({
                  onSourceChanged: () => {
                    selfieSegmentation.reset();
                  },
                  onFrame: async (input1, size) => {
                    await selfieSegmentation.send({image: input1});
                  },
                  examples: examples
                }),
                new controls.Slider({
                  title: 'Model Selection',
                  field: 'modelSelection',
                  discrete: ['General', 'Landscape'],
                }),
                new controls.Slider({
                  title: 'Effect',
                  field: 'effect',
                  discrete: {'background': 'Background', 'mask': 'Foreground'},
                }),
              ])
              .on(x => {
                const options = x;
                //videoElement.classList.toggle('selfie', options.selfieMode);
                activeEffect = x['effect'];
                selfieSegmentation.setOptions(options);
              });



    </script>
  </body>
</html>
